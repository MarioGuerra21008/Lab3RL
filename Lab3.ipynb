{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e75f95bc",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <td style=\"width:20%; vertical-align:middle;\">\n",
    "      <img src=\"LogoUVG.png\" width=\"400\"/>\n",
    "    </td>\n",
    "    <td style=\"text-align:left; vertical-align:middle;\">\n",
    "      <h2 style=\"margin-bottom: 0;\">Universidad del Valle de Guatemala - UVG</h2>\n",
    "      <h3 style=\"margin-top: 0;\">Facultad de Ingenier铆a - Computaci贸n</h3>\n",
    "      <p style=\"font-size: 16px; margin-bottom: 0; margin-top: -20px\">\n",
    "        <strong>Curso:</strong> CC3104 - Aprendizaje por Refuerzo \n",
    "        <strong>Secci贸n:</strong> 10\n",
    "      </p>\n",
    "      <p style=\"font-size: 16px; margin: 0;\"><strong>Laboratorio 3:</strong> MDP</p>\n",
    "      <br>\n",
    "      <p style=\"font-size: 15px; margin: 0;\"><strong>Autores:</strong></p>\n",
    "      <ul style=\"margin-top: 5px; padding-left: 20px; font-size: 15px;\">\n",
    "        <li>Diego Alexander Hern谩ndez Silvestre - <strong>21270</strong></li>\n",
    "        <li>Linda In茅s Jim茅nez Vides - <strong>21169</strong></li>\n",
    "        <li>Mario Antonio Guerra Morales - <strong>21008</strong></li>\n",
    "      </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3efa3ce",
   "metadata": {},
   "source": [
    "##  Task 1\n",
    "Responda a cada de las siguientes preguntas de forma clara y lo m谩s completamente posible.\n",
    "\n",
    "1. 驴Qu茅 es Programaci贸n Din谩mica y c贸mo se relaciona con RL?\n",
    "\n",
    "    La programaci贸n din谩mica es una t茅cnica de resoluci贸n de problemas complejos dividi茅ndolos en subrpoblemas m谩s simples, as铆 resuelve estos subproblemas una sola vez y almacena sus soluciones para poder reutilizarlas.\n",
    "\n",
    "    Se relaciona con RL ya que la DP contituye fundamentos te贸ricos y algor铆tmico de varios algoritmos empleados en aprendizaje por refuerzo. Estos se emplean para calcular pol铆ticas 贸ptimas dentro de un MDP cuando se conoce completamente el modelo del entorno.\n",
    "\n",
    "2. Explique en sus propias palabras el algoritmo de Iteraci贸n de P贸liza.\n",
    "\n",
    "    Es un m茅todo para encontrar la p贸liza 贸ptima de un agente en un ambiente conocido. Primero se tiene la evaluaci贸n de qu茅 tan buena es la poliza actual calculando la recompensa total esperada a largo plazo con esa p贸liza, conocida como la **evaluaci贸n de la p贸liza**. Una vez que se determina el valor de cada estado de la p贸liza actual se sigue con la mejora de esta, ajustando las reglas elegir la mejor acci贸n maximizando el valor esperado. Este 煤ltimo conocido como la **mejora de la p贸liza**\n",
    "    \n",
    "3. Explique en sus propias palabras el algoritmo de Iteraci贸n de Valor.\n",
    "\n",
    "    Tambi茅n es un m茅todo que busca encontrar la p贸liza 贸ptima, pero centr谩ndose directamente en calcular el valor 贸ptimo de cada posible estado, por lo que en lugar de definir una p贸liza y luego evaluarla, esta busca directamente el puntaje deseado m谩ximo qe se puede obtener desde cualquier estado, buscando siempre la mejor manera de actuar en el entorno.\n",
    "\n",
    "4. En el laboratorio pasado, vimos que el valor de los premios obtenidos se mantienen constantes, 驴por qu茅?\n",
    "\n",
    "    Los premios obtenidos se mantienen constantes porque el entorno est谩 definido de forma totalmente determinista, tanto en las transiciones (P) como en las recompensas (R). En cada simulaci贸n, se segu铆a una pol铆tica fija desde un estado inicial fijo, ejecutando siempre las mismas acciones en los mismos estados. Como resultado, todas las simulaciones generan exactamente la misma trayectoria, alcanzando siempre el estado objetivo con una recompensa acumulada constante."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "6349820d",
   "metadata": {},
   "source": [
    "##  Task 2\n",
    "\n",
    "El objetivo principal de este ejercicio es que simule un MDP que represente un robot que navega por un laberinto de cuadr铆culas de 3x3 y eval煤e una pol铆tica determinada. Por ello considere, a un robot navega por un laberinto de cuadr铆cula de 3x3. El robot puede moverse en cuatro direcciones: arriba, abajo, izquierda y derecha. El objetivo es navegar desde la posici贸n inicial hasta la posici贸n de meta evitando obst谩culos. El robot recibe una recompensa cuando alcanza la meta y una penalizaci贸n si choca con un obst谩culo.\n",
    "\n",
    "El laberinto es el siguiente:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Donde:\n",
    "\n",
    "- S = punto de inicio\n",
    "\n",
    "- G = punto de meta\n",
    "\n",
    "- X = son obst谩culos\n",
    "\n",
    "Instrucciones:\n",
    "\n",
    "- Defina los componentes del MDP:\n",
    "    - Estados: S = {0, 1, 2, 3, 4, 5, 6, 7, 8}, donde cada n煤mero representa una celda del laberinto.\n",
    "    - Acciones: A = {arriba, abajo, izquierda, derecha}\n",
    "    - Probabilidades de transici贸n: P(s' | s, a)\n",
    "    - Recompensas: R(s, a, s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b5ae27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "states = list(range(9)) # Lista de estados\n",
    "actions = ['up', 'down', 'left', 'right'] # Acciones que puede realizar.\n",
    "start_state = 0 # Estado inicial\n",
    "goal_state = 8 # Estado final\n",
    "obstacles = [2, 4] # Obst谩culos\n",
    "\n",
    "def state_to_coords(state):\n",
    "    return (state // 3, state % 3)\n",
    "\n",
    "def coords_to_state(row, col):\n",
    "    return row * 3 + col\n",
    "\n",
    "P = {} # Probabilidades de transici贸n\n",
    "R = {} # Recompensas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effb6a76",
   "metadata": {},
   "source": [
    "Matriz de transici贸n:\n",
    "\n",
    "- Defina las probabilidades de transici贸n P como un diccionario donde P[s][a] asigna los siguientes estados s' a sus probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2779cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in states:\n",
    "    if s in obstacles or s == goal_state:\n",
    "        continue\n",
    "    P[s] = {}\n",
    "    row, col = state_to_coords(s)\n",
    "\n",
    "    for a in actions:\n",
    "        P[s][a] = {}\n",
    "        next_row, next_col = row, col\n",
    "\n",
    "        if a == 'up': next_row -= 1\n",
    "        elif a == 'down': next_row += 1\n",
    "        elif a == 'left': next_col -= 1\n",
    "        elif a == 'right': next_col += 1\n",
    "\n",
    "        if 0 <= next_row < 3 and 0 <= next_col < 3:\n",
    "            s_prime = coords_to_state(next_row, next_col)\n",
    "            if s_prime in obstacles:\n",
    "                s_prime = s  # rebota\n",
    "        else:\n",
    "            s_prime = s  # rebota\n",
    "\n",
    "        P[s][a][s_prime] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22251c5d",
   "metadata": {},
   "source": [
    "Funci贸n de recompensa:\n",
    "\n",
    "- Defina las recompensas R como un diccionario donde R[s][a][s'] da la recompensa por la transici贸n del estado s al estado s' mediante la acci贸n a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46e2c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in states:\n",
    "    if s in obstacles or s == goal_state:\n",
    "        continue\n",
    "    R[s] = {}\n",
    "    row, col = state_to_coords(s)\n",
    "\n",
    "    for a in actions:\n",
    "        R[s][a] = {}\n",
    "\n",
    "        next_row, next_col = row, col\n",
    "        if a == 'up': next_row -= 1\n",
    "        elif a == 'down': next_row += 1\n",
    "        elif a == 'left': next_col -= 1\n",
    "        elif a == 'right': next_col += 1\n",
    "\n",
    "        if 0 <= next_row < 3 and 0 <= next_col < 3:\n",
    "            s_prime = coords_to_state(next_row, next_col)\n",
    "            if s_prime in obstacles:\n",
    "                s_prime = s\n",
    "                reward = -1\n",
    "            elif s_prime == goal_state:\n",
    "                reward = 10\n",
    "            else:\n",
    "                reward = -0.1\n",
    "        else:\n",
    "            s_prime = s\n",
    "            reward = -1\n",
    "\n",
    "        R[s][a][s_prime] = reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d65d9d",
   "metadata": {},
   "source": [
    "Inicializar funci贸n de valor:\n",
    "\n",
    "- Inicialice la funci贸n de valor V para todos los estados en 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2983635f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "V = np.zeros(len(states))\n",
    "print(V.reshape((3, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cab64f",
   "metadata": {},
   "source": [
    "Algoritmo de iteraci贸n de valor:\n",
    "\n",
    "- Implemente el algoritmo de iteraci贸n de valores para actualizar la funci贸n de valor V y encontrar la pol铆tica 贸ptima.\n",
    "- Usa un factor de descuento  de 0,9.\n",
    "- La iteraci贸n debe detenerse cuando el cambio m谩ximo en la funci贸n de valor sea menor que un umbral (por ejemplo, 0,001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "941418d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "theta = 0.001\n",
    "\n",
    "def value_iteration():\n",
    "    V = np.zeros(len(states))\n",
    "    policy = np.array([\"\"] * len(states))\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            if s in obstacles or s == goal_state:\n",
    "                continue\n",
    "            v = V[s]\n",
    "            max_value = float('-inf')\n",
    "            best_action = None\n",
    "            for a in actions:\n",
    "                total = 0\n",
    "                for s_prime in P[s][a]:\n",
    "                    total += P[s][a][s_prime] * (R[s][a][s_prime] + gamma * V[s_prime])\n",
    "                if total > max_value:\n",
    "                    max_value = total\n",
    "                    best_action = a\n",
    "            V[s] = max_value\n",
    "            policy[s] = best_action\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3683a139",
   "metadata": {},
   "source": [
    "Extraiga la pol铆tica 贸ptima de la iteraci贸n de valor:\n",
    "\n",
    "- Despu茅s de converger, extraiga la pol铆tica 贸ptima de la funci贸n de valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd81f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Funci贸n de valor 贸ptima (VI):\n",
      "[[ 7.019   6.2171  0.    ]\n",
      " [ 7.91    0.     10.    ]\n",
      " [ 8.9    10.      0.    ]]\n",
      "\n",
      " Pol铆tica 贸ptima (VI):\n",
      "[['d' 'l' '']\n",
      " ['d' '' 'd']\n",
      " ['r' 'r' '']]\n"
     ]
    }
   ],
   "source": [
    "V_vi, policy_vi = value_iteration()\n",
    "\n",
    "print(\"Funci贸n de valor 贸ptima (VI):\")\n",
    "print(V_vi.reshape((3, 3)))\n",
    "\n",
    "print(\"Pol铆tica 贸ptima (VI):\")\n",
    "print(policy_vi.reshape((3, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c6cf14",
   "metadata": {},
   "source": [
    "Algoritmo de iteraci贸n de pol铆ticas:\n",
    "\n",
    "- Implemente el algoritmo de iteraci贸n de pol铆ticas para encontrar la pol铆tica 贸ptima.\n",
    "- Inicialice una pol铆tica aleatoria.\n",
    "- Evaluaci贸n de pol铆ticas: eval煤e la pol铆tica actual para encontrar la funci贸n de valor.\n",
    "- Mejora de la pol铆tica: actualice la pol铆tica en funci贸n de la funci贸n de valor.\n",
    "- La iteraci贸n deber铆a detenerse cuando la pol铆tica ya no cambie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cec2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C贸digo aqu铆"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
